{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEEx2kk9YSBU",
        "outputId": "9174577c-b672-4e03-f91e-1707bc6aea30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/quake_adaptive_demo\n"
          ]
        }
      ],
      "source": [
        "!pip -q install numpy rich\n",
        "%mkdir -p quake_adaptive_demo\n",
        "%cd quake_adaptive_demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESHxSb7fYvr1",
        "outputId": "525585c9-d7fc-4aa0-fd79-601de7fe857c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing quake_min.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile quake_min.py\n",
        "from __future__ import annotations\n",
        "import math, time, random\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "import numpy as np\n",
        "\n",
        "def l2(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(np.sum((a - b) ** 2))\n",
        "\n",
        "def l2_batch(x: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
        "    x2 = np.sum(x*x, axis=1, keepdims=True)\n",
        "    y2 = np.sum(y*y, axis=1, keepdims=True).T\n",
        "    return x2 + y2 - 2 * (x @ y.T)\n",
        "\n",
        "def topk_indices(arr: np.ndarray, k: int) -> np.ndarray:\n",
        "    if k >= arr.shape[0]:\n",
        "        return np.argsort(arr)\n",
        "    idx = np.argpartition(arr, kth=k)[:k]\n",
        "    return idx[np.argsort(arr[idx])]\n",
        "\n",
        "def kmeans(x: np.ndarray, k: int, iters: int = 15, seed: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n, d = x.shape\n",
        "    centroids = x[rng.choice(n, size=k, replace=False)].copy()\n",
        "    assign = np.zeros(n, dtype=np.int32)\n",
        "    for _ in range(iters):\n",
        "        dists = l2_batch(x, centroids)\n",
        "        assign = np.argmin(dists, axis=1)\n",
        "        for j in range(k):\n",
        "            mask = assign == j\n",
        "            if np.any(mask):\n",
        "                centroids[j] = np.mean(x[mask], axis=0)\n",
        "            else:\n",
        "                centroids[j] = x[rng.integers(0, n)]\n",
        "    return centroids, assign\n",
        "\n",
        "@dataclass\n",
        "class BasePartition:\n",
        "    vecs: np.ndarray\n",
        "    ids: np.ndarray\n",
        "    centroid: np.ndarray\n",
        "    hits: int = 0\n",
        "    last_split_at: int = 0\n",
        "\n",
        "@dataclass\n",
        "class CoarseCell:\n",
        "    centroid: np.ndarray\n",
        "    base_ids: List[int] = field(default_factory=list)\n",
        "\n",
        "class AdaptiveIVF:\n",
        "    def __init__(self, dim: int, k_coarse: int = 16, k_base: int = 4, seed: int = 0):\n",
        "        self.dim = dim\n",
        "        self.k_coarse = k_coarse\n",
        "        self.k_base = k_base\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.coarse: List[CoarseCell] = []\n",
        "        self.base_parts: List[BasePartition] = []\n",
        "        self.id2loc: Dict[int, Tuple[int,int]] = {}\n",
        "        self.query_counter = 0\n",
        "        self.split_size = 3000\n",
        "        self.merge_size = 300\n",
        "        self.hot_split_multiplier = 1.5\n",
        "        self.cold_merge_multiplier = 1.5\n",
        "\n",
        "    def build(self, x: np.ndarray, ids: Optional[np.ndarray] = None, coarse_k: Optional[int] = None, base_k: Optional[int] = None):\n",
        "        if ids is None:\n",
        "            ids = np.arange(x.shape[0], dtype=np.int64)\n",
        "        if coarse_k: self.k_coarse = coarse_k\n",
        "        if base_k: self.k_base = base_k\n",
        "        coarse_centroids, coarse_assign = kmeans(x, self.k_coarse, iters=12, seed=42)\n",
        "        self.coarse = [CoarseCell(c) for c in coarse_centroids]\n",
        "        for c_id in range(self.k_coarse):\n",
        "            mask_c = coarse_assign == c_id\n",
        "            if not np.any(mask_c): continue\n",
        "            x_c = x[mask_c]\n",
        "            ids_c = ids[mask_c]\n",
        "            kb = min(self.k_base, max(1, x_c.shape[0] // 50))\n",
        "            base_centroids, base_assign = kmeans(x_c, kb, iters=10, seed=123 + c_id)\n",
        "            for b in range(kb):\n",
        "                mask_b = base_assign == b\n",
        "                if not np.any(mask_b): continue\n",
        "                xp = x_c[mask_b]\n",
        "                ip = ids_c[mask_b]\n",
        "                bp = BasePartition(vecs=xp, ids=ip, centroid=np.mean(xp, axis=0))\n",
        "                self.base_parts.append(bp)\n",
        "                bidx = len(self.base_parts) - 1\n",
        "                self.coarse[c_id].base_ids.append(bidx)\n",
        "                for off, _id in enumerate(ip):\n",
        "                    self.id2loc[int(_id)] = (bidx, off)\n",
        "\n",
        "    def insert(self, v: np.ndarray, _id: int):\n",
        "        c_d = np.array([l2(v, c.centroid) for c in self.coarse])\n",
        "        c_idx = int(np.argmin(c_d))\n",
        "        base_idxs = self.coarse[c_idx].base_ids\n",
        "        if not base_idxs:\n",
        "            bp = BasePartition(vecs=v[None,:], ids=np.array([_id]), centroid=v.copy())\n",
        "            self.base_parts.append(bp)\n",
        "            bidx = len(self.base_parts) - 1\n",
        "            self.coarse[c_idx].base_ids.append(bidx)\n",
        "            self.id2loc[_id] = (bidx, 0)\n",
        "            return\n",
        "        dists = [l2(v, self.base_parts[b].centroid) for b in base_idxs]\n",
        "        bidx = int(base_idxs[int(np.argmin(dists))])\n",
        "        bp = self.base_parts[bidx]\n",
        "        bp.vecs = np.vstack([bp.vecs, v])\n",
        "        bp.ids = np.append(bp.ids, _id)\n",
        "        bp.centroid = np.mean(bp.vecs, axis=0)\n",
        "        self.id2loc[_id] = (bidx, bp.vecs.shape[0]-1)\n",
        "\n",
        "    def delete(self, _id: int):\n",
        "        loc = self.id2loc.get(int(_id))\n",
        "        if loc is None: return\n",
        "        bidx, off = loc\n",
        "        bp = self.base_parts[bidx]\n",
        "        last = bp.vecs.shape[0]-1\n",
        "        bp.vecs[[off, last]] = bp.vecs[[last, off]]\n",
        "        bp.ids[[off, last]] = bp.ids[[last, off]]\n",
        "        bp.vecs = bp.vecs[:-1]\n",
        "        bp.ids = bp.ids[:-1]\n",
        "        if bp.vecs.shape[0] > 0:\n",
        "            bp.centroid = np.mean(bp.vecs, axis=0)\n",
        "        if off < bp.ids.shape[0]:\n",
        "            self.id2loc[int(bp.ids[off])] = (bidx, off)\n",
        "        self.id2loc.pop(int(_id), None)\n",
        "\n",
        "    def _partition_scores(self, q: np.ndarray):\n",
        "        centroids = np.array([bp.centroid for bp in self.base_parts])\n",
        "        d2 = l2_batch(q[None,:], centroids)[0]\n",
        "        size = np.array([bp.vecs.shape[0] for bp in self.base_parts])\n",
        "        tau = np.median(np.sqrt(d2)) + 1e-6\n",
        "        logits = -np.sqrt(d2) / (tau + 1e-6) + 0.5*np.log(size + 1.0)\n",
        "        m = np.max(logits)\n",
        "        p = np.exp(logits - m); p = p / np.sum(p)\n",
        "        out = [(i, float(p[i]), int(size[i])) for i in range(len(self.base_parts))]\n",
        "        out.sort(key=lambda t: t[1], reverse=True)\n",
        "        return out\n",
        "\n",
        "    def _choose_nprobe(self, probs, target_recall: float = 0.9, max_probe: int = 64) -> int:\n",
        "        cum = 0.0; n = 0\n",
        "        for _, pi, _ in probs:\n",
        "            cum += pi; n += 1\n",
        "            if cum >= target_recall: return n\n",
        "            if n >= max_probe: return n\n",
        "        return n\n",
        "\n",
        "    def search(self, q: np.ndarray, k: int = 10, target_recall: float = 0.9, exact_ref: Optional[np.ndarray] = None):\n",
        "        self.query_counter += 1\n",
        "        probs = self._partition_scores(q)\n",
        "        nprobe = self._choose_nprobe(probs, target_recall=target_recall)\n",
        "        cand_ids, cand_vecs = [], []\n",
        "        for i in range(nprobe):\n",
        "            bidx = probs[i][0]\n",
        "            bp = self.base_parts[bidx]\n",
        "            bp.hits += 1\n",
        "            cand_ids.append(bp.ids); cand_vecs.append(bp.vecs)\n",
        "        if len(cand_ids) == 0:\n",
        "            return np.array([]), np.array([]), {\"nprobe\": 0, \"scanned\": 0}\n",
        "        C_ids = np.concatenate(cand_ids)\n",
        "        C = np.vstack(cand_vecs)\n",
        "        d2 = l2_batch(q[None,:], C)[0]\n",
        "        topk = topk_indices(d2, min(k, d2.shape[0]))\n",
        "        found_ids = C_ids[topk]; found_d2 = d2[topk]\n",
        "        rec = None\n",
        "        if exact_ref is not None and exact_ref.size > 0:\n",
        "            inter = len(set(map(int, found_ids)) & set(map(int, exact_ref)))\n",
        "            rec = inter / max(1, min(k, exact_ref.size))\n",
        "        return found_ids, found_d2, {\"nprobe\": nprobe, \"scanned\": C.shape[0], \"recall_at_k\": rec}\n",
        "\n",
        "    def maintain(self, hot_qps_window: int = 2000):\n",
        "        for bidx, bp in enumerate(list(self.base_parts)):\n",
        "            size = bp.vecs.shape[0]\n",
        "            hotness = (bp.hits - bp.last_split_at)\n",
        "            split_thresh = self.split_size / max(1.0, (hotness / hot_qps_window))\n",
        "            split_thresh = max(self.split_size / self.hot_split_multiplier, min(self.split_size * 2, split_thresh))\n",
        "            if size >= split_thresh and size >= 16:\n",
        "                c, a = kmeans(bp.vecs, k=2, iters=8, seed=17 + bidx)\n",
        "                mask0 = a == 0; mask1 = ~mask0\n",
        "                if np.any(mask0) and np.any(mask1):\n",
        "                    bp0 = BasePartition(vecs=bp.vecs[mask0], ids=bp.ids[mask0], centroid=np.mean(bp.vecs[mask0], axis=0))\n",
        "                    bp1 = BasePartition(vecs=bp.vecs[mask1], ids=bp.ids[mask1], centroid=np.mean(bp.vecs[mask1], axis=0))\n",
        "                    self.base_parts[bidx] = bp0\n",
        "                    self.base_parts.append(bp1)\n",
        "                    new_idx = len(self.base_parts) - 1\n",
        "                    for off, _id in enumerate(bp0.ids): self.id2loc[int(_id)] = (bidx, off)\n",
        "                    for off, _id in enumerate(bp1.ids): self.id2loc[int(_id)] = (new_idx, off)\n",
        "                    self.base_parts[bidx].last_split_at = self.query_counter\n",
        "                    self.base_parts[new_idx].last_split_at = self.query_counter\n",
        "        tiny = [i for i, bp in enumerate(self.base_parts) if bp.vecs.shape[0] <= self.merge_size]\n",
        "        used = set()\n",
        "        for i in tiny:\n",
        "            if i in used: continue\n",
        "            ci = self.base_parts[i].centroid\n",
        "            best_j, best_d = None, float('inf')\n",
        "            for j in tiny:\n",
        "                if j == i or j in used: continue\n",
        "                cj = self.base_parts[j].centroid\n",
        "                d = l2(ci, cj)\n",
        "                if d < best_d:\n",
        "                    best_d, best_j = d, j\n",
        "            if best_j is None: continue\n",
        "            bpi = self.base_parts[i]\n",
        "            bpj = self.base_parts[best_j]\n",
        "            vecs = np.vstack([bpi.vecs, bpj.vecs])\n",
        "            ids = np.concatenate([bpi.ids, bpj.ids])\n",
        "            bpi.vecs, bpi.ids, bpi.centroid = vecs, ids, np.mean(vecs, axis=0)\n",
        "            for off, _id in enumerate(ids):\n",
        "                self.id2loc[int(_id)] = (i, off)\n",
        "            bpj.vecs = np.zeros((0, self.dim))\n",
        "            bpj.ids = np.zeros((0,), dtype=np.int64)\n",
        "\n",
        "    def exact_topk(self, q: np.ndarray, all_vecs: np.ndarray, all_ids: np.ndarray, k: int) -> np.ndarray:\n",
        "        d2 = l2_batch(q[None,:], all_vecs)[0]\n",
        "        topk = topk_indices(d2, min(k, d2.shape[0]))\n",
        "        return all_ids[topk]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUuORmTIY2WP",
        "outputId": "16562cbb-7fab-4e91-978c-a355e72b1b26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing run_demo.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile run_demo.py\n",
        "import time, math, random\n",
        "from typing import Tuple\n",
        "import numpy as np\n",
        "from rich import print, box\n",
        "from rich.table import Table\n",
        "from quake_min import AdaptiveIVF\n",
        "\n",
        "def make_dataset(n=50000, d=64, n_clusters=50, seed=7) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    centers = rng.normal(size=(n_clusters, d)) * 4.0\n",
        "    sizes = rng.multinomial(n, [1/n_clusters]*n_clusters)\n",
        "    xs = []\n",
        "    for i, sz in enumerate(sizes):\n",
        "        if sz == 0: continue\n",
        "        pts = centers[i] + rng.normal(size=(sz, d))\n",
        "        xs.append(pts)\n",
        "    X = np.vstack(xs).astype(np.float32)\n",
        "    ids = np.arange(X.shape[0], dtype=np.int64)\n",
        "    return X, ids\n",
        "\n",
        "def zipf_partition_sampler(P, alpha=1.2, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    ranks = np.arange(1, P+1)\n",
        "    weights = 1 / (ranks ** alpha)\n",
        "    weights = weights / weights.sum()\n",
        "    def sample():\n",
        "        return int(rng.choice(P, p=weights))\n",
        "    return sample, weights\n",
        "\n",
        "def main():\n",
        "    n = 40000\n",
        "    d = 64\n",
        "    k_coarse = 16\n",
        "    k_base = 4\n",
        "    queries = 500\n",
        "    k = 10\n",
        "    target_recall = 0.9\n",
        "    seed = 7\n",
        "\n",
        "    print(f\"[bold]Building dataset n={n}, d={d}[/bold]\")\n",
        "    X, ids = make_dataset(n=n, d=d, n_clusters=60, seed=seed)\n",
        "\n",
        "    print(f\"[bold]Building index (k_coarse={k_coarse}, k_base={k_base})[/bold]\")\n",
        "    idx = AdaptiveIVF(dim=d, k_coarse=k_coarse, k_base=k_base, seed=seed)\n",
        "    t0 = time.time()\n",
        "    idx.build(X, ids)\n",
        "    build_s = time.time() - t0\n",
        "    print(f\"Build time: {build_s:.2f}s, base partitions: {len(idx.base_parts)}\")\n",
        "\n",
        "    P = len(idx.base_parts)\n",
        "    sample_partition, weights = zipf_partition_sampler(P, alpha=1.1, seed=seed+1)\n",
        "    rng = np.random.default_rng(seed+2)\n",
        "\n",
        "    qlat, qrec, qnprobe, qscan = [], [], [], []\n",
        "    print(\"[bold]Running queries with APS + maintenance...[/bold]\")\n",
        "    for t in range(1, queries+1):\n",
        "        p = sample_partition()\n",
        "        bp = idx.base_parts[p]\n",
        "        if bp.vecs.shape[0] == 0:\n",
        "            v = X[rng.integers(0, X.shape[0])]\n",
        "        else:\n",
        "            v = bp.vecs[rng.integers(0, bp.vecs.shape[0])]\n",
        "        q = (v + rng.normal(size=v.shape)*0.1).astype(np.float32)\n",
        "        exact_ids = idx.exact_topk(q, X, ids, k=k)\n",
        "        t1 = time.time()\n",
        "        found_ids, found_d2, meta = idx.search(q, k=k, target_recall=target_recall, exact_ref=exact_ids)\n",
        "        dt = (time.time() - t1) * 1000.0\n",
        "        qlat.append(dt); qnprobe.append(meta[\"nprobe\"]); qscan.append(meta[\"scanned\"]); qrec.append(meta[\"recall_at_k\"] or 0.0)\n",
        "\n",
        "        if t % 20 == 0:\n",
        "            for _ in range(10):\n",
        "                v_new = rng.normal(size=(d,)).astype(np.float32) * 0.5 + rng.normal(size=(d,)).astype(np.float32)\n",
        "                new_id = int(ids.max() + rng.integers(1, 1000))\n",
        "                idx.insert(v_new, new_id)\n",
        "            for _ in range(10):\n",
        "                del_id = int(ids[rng.integers(0, ids.shape[0])])\n",
        "                idx.delete(del_id)\n",
        "\n",
        "        if t % 50 == 0:\n",
        "            idx.maintain()\n",
        "\n",
        "        if t % 100 == 0:\n",
        "            print(f\"[dim].. q{t}: latency {dt:.2f} ms, nprobe {meta['nprobe']}, scanned {meta['scanned']}, recall@{k}={meta['recall_at_k']:.2f}[/dim]\")\n",
        "\n",
        "    def mean(x): return float(np.mean(x))\n",
        "    def p(x,q): return float(np.percentile(x,q))\n",
        "\n",
        "    tbl = Table(title=\"Adaptive IVF Demo — Summary\", box=box.SIMPLE_HEAVY)\n",
        "    tbl.add_column(\"Metric\"); tbl.add_column(\"Value\")\n",
        "    tbl.add_row(\"Queries\", str(queries))\n",
        "    tbl.add_row(\"Avg latency (ms)\", f\"{mean(qlat):.2f}\")\n",
        "    tbl.add_row(\"p50 / p95 latency (ms)\", f\"{p(qlat,50):.2f} / {p(qlat,95):.2f}\")\n",
        "    tbl.add_row(\"Avg nprobe\", f\"{mean(qnprobe):.2f}\")\n",
        "    tbl.add_row(\"Avg vectors scanned\", f\"{mean(qscan):.0f}\")\n",
        "    tbl.add_row(f\"Avg recall@{k}\", f\"{mean(qrec):.3f}\")\n",
        "    print(); print(tbl)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKgqTNc4Y6GW",
        "outputId": "d1e75f62-07cf-4e15-acd6-88cab9a7b1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mBuilding dataset \u001b[0m\u001b[1;33mn\u001b[0m\u001b[1m=\u001b[0m\u001b[1;36m40000\u001b[0m\u001b[1m, \u001b[0m\u001b[1;33md\u001b[0m\u001b[1m=\u001b[0m\u001b[1;36m64\u001b[0m\n",
            "\u001b[1mBuilding index \u001b[0m\u001b[1m(\u001b[0m\u001b[1;33mk_coarse\u001b[0m\u001b[1m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1m, \u001b[0m\u001b[1;33mk_base\u001b[0m\u001b[1m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\n",
            "Build time: \u001b[1;36m0.\u001b[0m54s, base partitions: \u001b[1;36m64\u001b[0m\n",
            "\u001b[1mRunning queries with APS + maintenance\u001b[0m\u001b[1;33m...\u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:130: RuntimeWarning: invalid value encountered in divide\n",
            "  ret = um.true_divide(\n",
            "\u001b[2m.. q100: latency \u001b[0m\u001b[1;2;36m12.05\u001b[0m\u001b[2m ms, nprobe \u001b[0m\u001b[1;2;36m64\u001b[0m\u001b[2m, scanned \u001b[0m\u001b[1;2;36m36556\u001b[0m\u001b[2m, recall@\u001b[0m\u001b[1;2;36m10\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2;36m.00\u001b[0m\n",
            "\u001b[2m.. q200: latency \u001b[0m\u001b[1;2;36m18.67\u001b[0m\u001b[2m ms, nprobe \u001b[0m\u001b[1;2;36m64\u001b[0m\u001b[2m, scanned \u001b[0m\u001b[1;2;36m35206\u001b[0m\u001b[2m, recall@\u001b[0m\u001b[1;2;36m10\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2;36m.00\u001b[0m\n",
            "\u001b[2m.. q300: latency \u001b[0m\u001b[1;2;36m12.21\u001b[0m\u001b[2m ms, nprobe \u001b[0m\u001b[1;2;36m64\u001b[0m\u001b[2m, scanned \u001b[0m\u001b[1;2;36m35215\u001b[0m\u001b[2m, recall@\u001b[0m\u001b[1;2;36m10\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2;36m.00\u001b[0m\n",
            "\u001b[2m.. q400: latency \u001b[0m\u001b[1;2;36m11.69\u001b[0m\u001b[2m ms, nprobe \u001b[0m\u001b[1;2;36m64\u001b[0m\u001b[2m, scanned \u001b[0m\u001b[1;2;36m35225\u001b[0m\u001b[2m, recall@\u001b[0m\u001b[1;2;36m10\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2;36m.00\u001b[0m\n",
            "\u001b[2m.. q500: latency \u001b[0m\u001b[1;2;36m11.17\u001b[0m\u001b[2m ms, nprobe \u001b[0m\u001b[1;2;36m64\u001b[0m\u001b[2m, scanned \u001b[0m\u001b[1;2;36m35228\u001b[0m\u001b[2m, recall@\u001b[0m\u001b[1;2;36m10\u001b[0m\u001b[2m=\u001b[0m\u001b[1;2;36m1\u001b[0m\u001b[1;2;36m.00\u001b[0m\n",
            "\n",
            "\u001b[3m       Adaptive IVF Demo — Summary        \u001b[0m\n",
            "                                          \n",
            " \u001b[1m \u001b[0m\u001b[1mMetric                \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mValue        \u001b[0m\u001b[1m \u001b[0m \n",
            " ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \n",
            "  Queries                  500            \n",
            "  Avg latency (ms)         14.20          \n",
            "  p50 / p95 latency (ms)   12.11 / 24.43  \n",
            "  Avg nprobe               62.70          \n",
            "  Avg vectors scanned      35650          \n",
            "  Avg recall@10            0.968          \n",
            "                                          \n"
          ]
        }
      ],
      "source": [
        "!python run_demo.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
